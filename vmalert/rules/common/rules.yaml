---
apiVersion: operator.victoriametrics.com/v1beta1
kind: VMRule
metadata:
  name: common
  labels:
    app.kubernetes.io/component: vmrule
spec:
  groups:
    - name: vm-health
      rules:
        - alert: TooManyRestarts
          expr: changes(process_start_time_seconds{job=~".*(victoriametrics|vmselect|vminsert|vmstorage|vmagent|vmalert|vmsingle|vmalertmanager|vmauth).*"}[15m]) > 2
          labels:
            severity: critical
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            summary: "{{ $labels.job }} too many restarts (instance {{ $labels.instance }})"
            description: "Job {{ $labels.job }} (instance {{ $labels.instance }}) has restarted more than twice in the last 15 minutes.
              It might be crashlooping."

        - alert: ServiceDown
          expr: up{job=~"(victoriametrics|vmagent|vmalert|billing|vmadmin|provision).*"} == 0
          for: 3m
          labels:
            severity: critical
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            summary: "Service `{{ $labels.job }}` is down on `{{ $labels.instance }}`"
            description: "`{{ $labels.instance }}` of job `{{ $labels.job }}` has been down for more than 3 minutes."

        - alert: ProcessNearFDLimits
          expr: (process_max_fds {namespace="monitoring"}- process_open_fds{namespace="monitoring"}) < 100
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: 'Number of free file descriptors is less than 100 for "{{ $labels.job }}"("{{ $labels.instance }}") for the last 5m'
            description:
              "Exhausting OS file descriptors limit can cause severe degradation of the process.
              Consider to increase the limit as fast as possible."

        - alert: TooHighMemoryUsage
          expr: (min_over_time(process_resident_memory_anon_bytes{namespace="monitoring"}[10m]) / vm_available_memory_bytes{namespace="monitoring"}) > 0.8
          for: 5m
          labels:
            severity: critical
            service: "{{ $labels.job }}"
            instance: "{{ $labels.instance }}"
          annotations:
            summary: 'It is more than 80% of memory used by "{{ $labels.job }}"("{{ $labels.instance }}")'
            description:
              "Too high memory usage may result into multiple issues such as OOMs or degraded performance.
              Consider to either increase available memory or decrease the load on the process."

        - alert: TooHighCPUUsage
          expr: rate(process_cpu_seconds_total{namespace="monitoring"}[5m]) / process_cpu_cores_available{namespace="monitoring"} > 0.9
          for: 5m
          labels:
            severity: critical
            service: "{{ $labels.job }}"
            instance: "{{ $labels.instance }}"
          annotations:
            summary: "More than 90% of CPU is used by \"{{ $labels.job }}\"(\"{{ $labels.instance }}\") during the last 5m"
            description: "Too high CPU usage may be a sign of insufficient resources and make process unstable.
                Consider to either increase available CPU resources or decrease the load on the process."

    # Alerts group for VM single assumes that Grafana dashboard
    # https://grafana.com/grafana/dashboards/10229 is installed.
    # Pls update the `dashboard` annotation according to your setup.
    - name: vmsingle
      interval: 30s
      concurrency: 2
      rules:

        - alert: DiskRunsOutOfSpaceIn3Days
          expr: |
            vm_free_disk_space_bytes {namespace="monitoring"}/ ignoring(path)
            (
                rate(vm_rows_added_to_storage_total{namespace="monitoring"}[1d])
              * scalar(
                sum(vm_data_size_bytes{namespace="monitoring",type!="indexdb"}) /
                sum(vm_rows{namespace="monitoring",type!="indexdb"})
                )
            ) < 3 * 24 * 3600
          for: 30m
          labels:
            severity: critical
            service: "{{ $labels.job }}"
            instance: "{{ $labels.instance }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/rTWCSo77z?viewPanel=73&var-instance={{ $labels.instance }}"
            summary: "Instance {{ $labels.instance }} will run out of disk space soon"
            description:
              "Taking into account current ingestion rate, free disk space will be enough only
              for {{ $value | humanizeDuration }} on instance {{ $labels.instance }}.\n
              Consider to limit the ingestion rate, decrease retention or scale the disk space if possible."

        - alert: ReadOnlyMode
          expr: vm_storage_is_read_only{namespace="monitoring"} > 0
          for: 5m
          labels:
            severity: critical
            service: "{{ $labels.job }}"
            instance: "{{ $labels.instance }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/rTWCSo77z?viewPanel=53&var-instance={{ $labels.instance }}"
            summary: "Instance {{ $labels.instance }} switched to read-only mode"
            description: "Disk utilisation on instance {{ $labels.instance }} is more than read-only threshold. Allocate more disk space"

        - alert: DiskRunsOutOfSpace
          expr: |
            sum(vm_data_size_bytes{namespace="monitoring"}) by(job, instance) /
            (
              sum(vm_free_disk_space_bytes{namespace="monitoring"}) by(job, instance) +
              sum(vm_data_size_bytes{namespace="monitoring"}) by(job, instance)
            ) > 0.8
          for: 30m
          labels:
            severity: critical
            service: "{{ $labels.job }}"
            instance: "{{ $labels.instance }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/rTWCSo77z?viewPanel=53&var-instance={{ $labels.instance }}"
            summary: "Instance {{ $labels.instance }} (job={{ $labels.job }}) will run out of disk space soon"
            description:
              "Disk utilisation on instance {{ $labels.instance }} is more than 80%.\n
              Having less than 20% of free disk space could cripple merges processes and overall performance.
              Consider to limit the ingestion rate, decrease retention or scale the disk space if possible."

        - alert: RequestErrorsToAPI
          expr: increase(vm_http_request_errors_total{namespace="monitoring"}[5m]) > 0
          for: 15m
          labels:
            severity: warning
            service: "{{ $labels.job }}"
            instance: "{{ $labels.instance }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/rTWCSo77z?viewPanel=35&var-instance={{ $labels.instance }}"
            summary: "Too many errors served for path {{ $labels.path }} (instance {{ $labels.instance }})"
            description:
              "Requests to path {{ $labels.path }} are receiving errors.
              Please verify if clients are sending correct requests."

        - alert: ConcurrentFlushesHitTheLimit
          expr: avg_over_time(vm_concurrent_insert_current{namespace="monitoring"}[1m]) >= vm_concurrent_insert_capacity
          for: 15m
          labels:
            severity: warning
            service: "{{ $labels.job }}"
            instance: "{{ $labels.instance }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/rTWCSo77z?viewPanel=59&var-instance={{ $labels.instance }}"
            summary: "VictoriaMetrics on instance {{ $labels.instance }} is constantly hitting concurrent flushes limit"
            description:
              "The limit of concurrent flushes on instance {{ $labels.instance }} is equal to number of CPUs.\n
              When VictoriaMetrics constantly hits the limit it means that storage is overloaded and requires more CPU."

        - alert: TooManyLogs
          expr: sum(increase(vm_log_messages_total{level!="info",namespace="monitoring"}[5m])) by (job, instance) > 0
          for: 15m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/rTWCSo77z?viewPanel=67&var-instance={{ $labels.instance }}"
            summary: 'Too many logs printed for job "{{ $labels.job }}" ({{ $labels.instance }})'
            description:
              "Logging rate for job \"{{ $labels.job }}\" ({{ $labels.instance }}) is {{ $value }} for last 15m.\n
              Worth to check logs for specific error messages."

        - alert: RowsRejectedOnIngestion
          expr: sum(rate(vm_rows_ignored_total{namespace="monitoring"}[5m])) by (instance, reason) > 0
          for: 15m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/rTWCSo77z?viewPanel=58&var-instance={{ $labels.instance }}"
            summary: 'Some rows are rejected on "{{ $labels.instance }}" on ingestion attempt'
            description:
              'VM is rejecting to ingest rows on "{{ $labels.instance }}" due to the
              following reason: "{{ $labels.reason }}"'

        - alert: TooHighChurnRate
          expr: |
            (
                sum(rate(vm_new_timeseries_created_total{namespace="monitoring"}[5m])) by(instance)
                /
                sum(rate(vm_rows_inserted_total{namespace="monitoring"}[5m])) by (instance)
              ) > 0.1
          for: 15m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/rTWCSo77z?viewPanel=66&var-instance={{ $labels.instance }}"
            summary: 'Churn rate is more than 10% on "{{ $labels.instance }}" for the last 15m'
            description:
              "VM constantly creates new time series on \"{{ $labels.instance }}\".\n
              This effect is known as Churn Rate.\n
              High Churn Rate tightly connected with database performance and may
              result in unexpected OOM's or slow queries."

        - alert: TooHighChurnRate24h
          expr: |
            sum(increase(vm_new_timeseries_created_total{namespace="monitoring"}[24h])) by(instance)
            >
            (sum(vm_cache_entries{namespace="monitoring",type="storage/hour_metric_ids"}) by(instance) * 3)
          for: 15m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/rTWCSo77z?viewPanel=66&var-instance={{ $labels.instance }}"
            summary: 'Too high number of new series on "{{ $labels.instance }}" created over last 24h'
            description:
              "The number of created new time series over last 24h is 3x times higher than
              current number of active series on \"{{ $labels.instance }}\".\n
              This effect is known as Churn Rate.\n
              High Churn Rate tightly connected with database performance and may
              result in unexpected OOM's or slow queries."

        - alert: TooHighSlowInsertsRate
          expr: |
            (
                sum(rate(vm_slow_row_inserts_total{namespace="monitoring"}[5m])) by(instance)
                /
                sum(rate(vm_rows_inserted_total{namespace="monitoring"}[5m])) by (instance)
              ) > 0.5
          for: 15m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/rTWCSo77z?viewPanel=68&var-instance={{ $labels.instance }}"
            summary: 'Percentage of slow inserts is more than 50% on "{{ $labels.instance }}" for the last 15m'
            description:
              'High rate of slow inserts on "{{ $labels.instance }}" may be a sign of resource exhaustion
              for the current load. It is likely more RAM is needed for optimal handling of the current number of active time series.'

        - alert: LabelsLimitExceededOnIngestion
          expr: sum(increase(vm_metrics_with_dropped_labels_total{namespace="monitoring"}[5m])) by (instance) > 0
          for: 15m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/oS7Bi_0Wz?viewPanel=74&var-instance={{ $labels.instance }}"
            summary: "Metrics ingested in ({{ $labels.instance }}) are exceeding labels limit"
            description:
              "VictoriaMetrics limits the number of labels per each metric with `-maxLabelsPerTimeseries` command-line flag.\n
              This prevents from ingesting metrics with too many labels. Please verify that `-maxLabelsPerTimeseries` is configured
              correctly or that clients which send these metrics aren't misbehaving."

    - name: vmauth
      interval: 30s
      rules:

        - alert: concurrentRequestsLimitReached
          expr: sum(rate(vmauth_concurrent_requests_limit_reached_total)) by (instance) > 0
          for: 1m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
          annotations:
            summary: "vmauth reached per total concurrent requests limit: {{ $labels.instance }}"
            description: "deploy additional vmauth."

        - alert: highCPUUSage
          for: 2m
          expr: sum(irate(container_cpu_usage_seconds_total{namespace=~"dbaas|dbaas-test", pod=~"vmauth.*",container="vmauth"}[5m]))by(pod) / sum(kube_pod_container_resource_limits{resource="cpu",unit="core",namespace=~"dbaas|dbaas-test", pod=~"vmauth.*",container="vmauth"})by(pod) > 0.8
          labels:
            severity: critical
            pod: "{{ $labels.pod }}"
          annotations:
            summary: "vmauth pod has significant CPU utilization, increase it's limit or replica count"
            description: "vmauth pod: {{ $labels.pod }} has CPU utilization: {{ $value }}"

    - name: vmalert
      rules:

        - alert: configReloadError
          expr: vmalert_config_last_reload_successful == 0
          for: 2m
          labels:
            severity: critical
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            summary: "vmalert last config reload error, you have to check it"
            description: "vmalert pod: {{ $labels.pod }} has incorrect config file"

        - alert: AlertingRulesError
          expr: sum(increase(vmalert_alerting_rules_errors_total{namespace="monitoring"}[5m])) without(alertname, id) > 0
          for: 5m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/LzldHAVnz?viewPanel=13&var-instance={{ $labels.instance }}&var-group={{ $labels.group }}"
            summary: "Alerting rules are failing for vmalert instance {{ $labels.instance }}"
            description: "Alerting rules execution is failing for group \"{{ $labels.group }}\".
              Check vmalert's logs for detailed error message."

        - alert: RecordingRulesError
          expr: sum(increase(vmalert_recording_rules_errors_total{namespace="monitoring"}[5m])) without(recording, id) > 0
          for: 5m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/LzldHAVnz?viewPanel=30&var-instance={{ $labels.instance }}&var-group={{ $labels.group }}"
            summary: "Recording rules are failing for vmalert instance {{ $labels.instance }}"
            description: "Recording rules execution is failing for group \"{{ $labels.group }}\".
              Check vmalert's logs for detailed error message."

        - alert: TooManyMissedIterations
          expr: increase(vmalert_iteration_missed_total{namespace="monitoring"}[5m]) > 0
          for: 15m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            summary: "vmalert instance {{ $labels.instance }} is missing rules evaluations"
            description: "vmalert instance {{ $labels.instance }} is missing rules evaluations for group \"{{ $labels.group }}\".
              The group evaluation time takes longer than the configured evaluation interval. This may result in missed
              alerting notifications or recording rules samples. Try increasing evaluation interval or concurrency of
              group \"{{ $labels.group }}\". See https://docs.victoriametrics.com/vmalert/#groups.
              If rule expressions are taking longer than expected, please see https://docs.victoriametrics.com/troubleshooting/#slow-queries."

        - alert: RemoteWriteErrors
          expr: increase(vmalert_remotewrite_errors_total{namespace="monitoring"}[5m]) > 0
          for: 15m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            summary: "vmalert instance {{ $labels.instance }} is failing to push metrics to remote write URL"
            description: "vmalert instance {{ $labels.instance }} is failing to push metrics generated via alerting
              or recording rules to the configured remote write URL. Check vmalert's logs for detailed error message."

        - alert: AlertmanagerErrors
          expr: increase(vmalert_alerts_send_errors_total{namespace="monitoring"}[5m]) > 0
          for: 15m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            summary: "vmalert instance {{ $labels.instance }} is failing to send notifications to Alertmanager"
            description: "vmalert instance {{ $labels.instance }} is failing to send alert notifications to \"{{ $labels.addr }}\".
              Check vmalert's logs for detailed error message."

    - name: alertmanager
      rules:

        - alert: configReloadError
          expr: alertmanager_config_last_reload_successful == 0
          for: 2m
          labels:
            severity: error
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            summary: "alertmanger has errors at config file"
            description: "alertmanager pod: {{ $labels.pod}} has invalid config"

        - alert: notifierErrors
          expr: sum(rate(alertmanager_notifications_failed_total[1m])) by (job, pod, integration)  >  0
          for: 2m
          labels:
            resource: "{{ $labels.integration }}"
            service: "{{ $labels.job }}"
            severity: warning
          annotations:
            summary: "alertmanager failed to send notification via integration"
            details: "alertmanager pod: {{ $labels.pod}} failed to send notification to the integration: {{ $labels.integration }}, check it's logs"

    # Alerts group for vmagent assumes that Grafana dashboard
    # https://grafana.com/grafana/dashboards/12683 is installed.
    # Pls update the `dashboard` annotation according to your setup.
    - name: vmagent
      interval: 30s
      concurrency: 2
      rules:

        - alert: PersistentQueueIsDroppingData
          expr: sum(increase(vm_persistentqueue_bytes_dropped_total{namespace="monitoring"}[5m])) by (job, instance) > 0
          for: 10m
          labels:
            severity: critical
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/dlE9IT7nk?viewPanel=49&var-instance={{ $labels.instance }}"
            summary: "Instance {{ $labels.instance }} is dropping data from persistent queue"
            description:
              "Vmagent dropped {{ $value | humanize1024 }} from persistent queue
              on instance {{ $labels.instance }} for the last 10m."

        - alert: RejectedRemoteWriteDataBlocksAreDropped
          expr: sum(increase(vmagent_remotewrite_packets_dropped_total{namespace="monitoring"}[5m])) without (url) > 0
          for: 15m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/G7Z9GzMGz?viewPanel=79&var-instance={{ $labels.instance }}"
            summary: "Job \"{{ $labels.job }}\" on instance {{ $labels.instance }} drops the rejected by
            remote-write server data blocks. Check the logs to find the reason for rejects."

        - alert: TooManyScrapeErrors
          expr: sum(increase(vm_promscrape_scrapes_failed_total{namespace="monitoring"}[5m])) by (job, instance) > 0
          for: 15m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/dlE9IT7nk?viewPanel=31&var-instance={{ $labels.instance }}"
            summary: 'Job "{{ $labels.job }}" on instance {{ $labels.instance }} fails to scrape targets for last 15m'

        - alert: TooManyWriteErrors
          expr: |
            (sum(increase(vm_ingestserver_request_errors_total{namespace="monitoring"}[5m])) by (job, instance)
            +
            sum(increase(vmagent_http_request_errors_total{namespace="monitoring"}[5m])) by (job, instance)) > 0
          for: 15m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/dlE9IT7nk?viewPanel=77&var-instance={{ $labels.instance }}"
            summary: 'Job "{{ $labels.job }}" on instance {{ $labels.instance }} responds with errors to write requests for last 15m.'

        - alert: TooManyRemoteWriteErrors
          expr: sum(rate(vmagent_remotewrite_retries_count_total{namespace="monitoring"}[5m])) by(job, instance, url) > 0
          for: 15m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/dlE9IT7nk?viewPanel=61&var-instance={{ $labels.instance }}"
            summary: 'Job "{{ $labels.job }}" on instance {{ $labels.instance }} fails to push to remote storage'
            description:
              "Vmagent fails to push data via remote write protocol to destination \"{{ $labels.url }}\"\n
              Ensure that destination is up and reachable."

        - alert: RemoteWriteConnectionIsSaturated
          expr: |
            (
            rate(vmagent_remotewrite_send_duration_seconds_total{namespace="monitoring"}[5m])
            /
            vmagent_remotewrite_queues{namespace="monitoring"}
            ) > 0.9
          for: 15m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/G7Z9GzMGz?viewPanel=84&var-instance={{ $labels.instance }}"
            summary: "Remote write connection from \"{{ $labels.job }}\" (instance {{ $labels.instance }}) to {{ $labels.url }} is saturated"
            description: "The remote write connection between vmagent \"{{ $labels.job }}\" (instance {{ $labels.instance }}) and destination \"{{ $labels.url }}\"
              is saturated by more than 90% and vmagent won't be able to keep up.\n
              This usually means that `-remoteWrite.queues` command-line flag must be increased in order to increase
              the number of connections per each remote storage."

        - alert: PersistentQueueForWritesIsSaturated
          expr: rate(vm_persistentqueue_write_duration_seconds_total{namespace="monitoring"}[5m]) > 0.9
          for: 15m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/G7Z9GzMGz?viewPanel=98&var-instance={{ $labels.instance }}"
            summary: "Persistent queue writes for instance {{ $labels.instance }} are saturated"
            description: "Persistent queue writes for vmagent \"{{ $labels.job }}\" (instance {{ $labels.instance }})
              are saturated by more than 90% and vmagent won't be able to keep up with flushing data on disk.
              In this case, consider to decrease load on the vmagent or improve the disk throughput."

        - alert: PersistentQueueForReadsIsSaturated
          expr: rate(vm_persistentqueue_read_duration_seconds_total{namespace="monitoring"}[5m]) > 0.9
          for: 15m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/G7Z9GzMGz?viewPanel=99&var-instance={{ $labels.instance }}"
            summary: "Persistent queue reads for instance {{ $labels.instance }} are saturated"
            description: "Persistent queue reads for vmagent \"{{ $labels.job }}\" (instance {{ $labels.instance }})
              are saturated by more than 90% and vmagent won't be able to keep up with reading data from the disk.
              In this case, consider to decrease load on the vmagent or improve the disk throughput."

        - alert: ConfigurationReloadFailure
          expr: |
            vm_promscrape_config_last_reload_successful{namespace="monitoring"} != 1
            or
            vmagent_relabel_config_last_reload_successful{namespace="monitoring"} != 1
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            summary: "Configuration reload failed for vmagent instance {{ $labels.instance }}"
            description: "Configuration hot-reload failed for vmagent on instance {{ $labels.instance }}.
            Check vmagent's logs for detailed error message."

    - name: backupmanager
      rules:

        - alert: BackupManagerErrors
          expr: increase(vm_backup_errors_total[1m]) > 0
          for: 10m
          labels:
            severity: critical
            # TODO: @ampert @dk uncomment it after alertmanager routing bugfix
            # customer_id: "{{ $labels.customer_id }}"
            resource_id: "{{ $labels.resource_id }}"
            type: "{{ $labels.type }}"
          annotations:
            summary: "{{ $labels.job }} backupmanager-{{ $labels.instance}} has fails for backups"
            description: "Job {{ $labels.job }} has fails for backups"

        - alert: BackupManagerErrorLogs
          # TODO: @ampert @dk add customer_id it after alertmanager routing bugfix
          expr: sum(rate(vm_log_messages_total{level="error",container="vmbackuper"}[5m])) by (job, instance,resource_id) > 0
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/gF-lxRdVz/victoriametrics-backupmanager?orgId=1&refresh=1m&var-instance={{ $labels.instance }}"
            summary: 'Too many logs printed for vmbackup "{{ $labels.job }}" ({{ $labels.instance }})'
            description:
              "Error log rate for vmbackup job \"{{ $labels.job }}\" ({{ $labels.instance }}) is {{ $value }}.\n
              Worth to check logs for specific error messages."

    - name: operator
      rules:

        - alert: logErrors
          expr: rate(operator_log_messages_total{level="error"}) > 0
          for: 15m
          labels:
            severity: high
          annotations:
            description: "operator has too many errors at logs: {{ $value}} check it logs"
            dashboard: "{{ $externalURL }}/d/1H179hunk/victoriametrics-operator?ds={{ $labels.dc }}&orgId=1"

        - alert: reconcileErrors
          expr: rate(controller_runtime_reconcile_errors_total{job="vm-operator-victoria-metrics-operator"}) > 0
          for: 10m
          labels:
            severity: high
            controller: "{{ $labels.controller }}"
          annotations:
            description: "operator cannot parse response from k8s api server, possible bug: {{ $value }}, check it logs"
            dashboard: "{{ $externalURL }}/d/1H179hunk/victoriametrics-operator?ds={{ $labels.dc }}&orgId=1"

        - alert: highQueueDepth
          expr: workqueue_depth{job="vm-operator-victoria-metrics-operator", name!~"(vmuser|vmalertmanagerconfig)"} > 10
          for: 15m
          labels:
            severity: high
            resource: "{{ $labels.name }}"
          annotations:
            description: "operator cannot handle reconciliation load for controller: `{{- $labels.name }}` current depth: {{ $value }}"
            dashboard: "{{ $externalURL }}/d/1H179hunk/victoriametrics-operator?ds={{ $labels.dc }}&orgId=1&viewPanel=20"

        - alert: highQueueDepthVMUserOrAMConfig
          expr: workqueue_depth{job="vm-operator-victoria-metrics-operator", name=~"(vmuser|vmalertmanagerconfig)"} > 200
          for: 30m
          labels:
            severity: high
            resource: "{{ $labels.name }}"
          annotations:
            description: "operator cannot handle reconciliation load for controller: `{{- $labels.name }}` current depth: {{ $value }}"
            dashboard: "{{ $externalURL }}/d/1H179hunk/victoriametrics-operator?ds={{ $labels.dc }}&orgId=1&viewPanel=20"

        - alert: BadObjects
          expr: |
            sum(
              operator_controller_bad_objects_count{job=~".*((victoria.*)|vm)-?operator"}
            ) by(controller) > 0
          for: 15m
          labels:
            severity: high
            controller: "{{ $labels.controller }}"
          annotations:
            description: "Operator got incorrect resources in controller {{ $labels.controller }}, check operator logs"
            dashboard: "{{ $externalURL }}/d/1H179hunk/victoriametrics-operator?ds={{ $labels.dc }}&orgId=1"
            summary: "Incorrect `{{ $labels.controller }}` resources in the cluster"
