---
apiVersion: operator.victoriametrics.com/v1beta1
kind: VMRule
metadata:
  name: common
  labels:
    app.kubernetes.io/component: vmrule
spec:
  groups:
    - name: vm-health
      rules:
        - alert: TooManyRestarts
          expr: changes(process_start_time_seconds{job=~".*(victoriametrics|vmselect|vminsert|vmstorage|vmagent|vmalert|vmsingle|vmalertmanager|vmauth).*"}[15m]) > 2
          labels:
            severity: critical
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            summary: "{{ $labels.job }} too many restarts (instance {{ $labels.instance }})"
            description: "Job {{ $labels.job }} (instance {{ $labels.instance }}) has restarted more than twice in the last 15 minutes.
              It might be crashlooping."

        - alert: ServiceDown
          expr: up{job=~"(victoriametrics|vmagent|vmalert|billing|vmadmin|provision).*"} == 0
          for: 3m
          labels:
            severity: critical
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            summary: "Service `{{ $labels.job }}` is down on `{{ $labels.instance }}`"
            description: "`{{ $labels.instance }}` of job `{{ $labels.job }}` has been down for more than 3 minutes."

        - alert: ProcessNearFDLimits
          expr: (process_max_fds {namespace="monitoring"}- process_open_fds{namespace="monitoring"}) < 100
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: 'Number of free file descriptors is less than 100 for "{{ $labels.job }}"("{{ $labels.instance }}") for the last 5m'
            description:
              "Exhausting OS file descriptors limit can cause severe degradation of the process.
              Consider to increase the limit as fast as possible."

        - alert: TooHighMemoryUsage
          expr: (min_over_time(process_resident_memory_anon_bytes{namespace="monitoring"}[10m]) / vm_available_memory_bytes{namespace="monitoring"}) > 0.8
          for: 5m
          labels:
            severity: critical
            service: "{{ $labels.job }}"
            instance: "{{ $labels.instance }}"
          annotations:
            summary: 'It is more than 80% of memory used by "{{ $labels.job }}"("{{ $labels.instance }}")'
            description:
              "Too high memory usage may result into multiple issues such as OOMs or degraded performance.
              Consider to either increase available memory or decrease the load on the process."

        - alert: TooHighCPUUsage
          expr: rate(process_cpu_seconds_total{namespace="monitoring"}[5m]) / process_cpu_cores_available{namespace="monitoring"} > 0.9
          for: 5m
          labels:
            severity: critical
            service: "{{ $labels.job }}"
            instance: "{{ $labels.instance }}"
          annotations:
            summary: "More than 90% of CPU is used by \"{{ $labels.job }}\"(\"{{ $labels.instance }}\") during the last 5m"
            description: "Too high CPU usage may be a sign of insufficient resources and make process unstable.
                Consider to either increase available CPU resources or decrease the load on the process."

    - name: vmalert
      rules:

        - alert: configReloadError
          expr: vmalert_config_last_reload_successful == 0
          for: 2m
          labels:
            severity: critical
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            summary: "vmalert last config reload error, you have to check it"
            description: "vmalert pod: {{ $labels.pod }} has incorrect config file"

        - alert: AlertingRulesError
          expr: sum(increase(vmalert_alerting_rules_errors_total{namespace="monitoring"}[5m])) without(alertname, id) > 0
          for: 5m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/LzldHAVnz?viewPanel=13&var-instance={{ $labels.instance }}&var-group={{ $labels.group }}"
            summary: "Alerting rules are failing for vmalert instance {{ $labels.instance }}"
            description: "Alerting rules execution is failing for group \"{{ $labels.group }}\".
              Check vmalert's logs for detailed error message."

        - alert: RecordingRulesError
          expr: sum(increase(vmalert_recording_rules_errors_total{namespace="monitoring"}[5m])) without(recording, id) > 0
          for: 5m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/LzldHAVnz?viewPanel=30&var-instance={{ $labels.instance }}&var-group={{ $labels.group }}"
            summary: "Recording rules are failing for vmalert instance {{ $labels.instance }}"
            description: "Recording rules execution is failing for group \"{{ $labels.group }}\".
              Check vmalert's logs for detailed error message."

        - alert: TooManyMissedIterations
          expr: increase(vmalert_iteration_missed_total{namespace="monitoring"}[5m]) > 0
          for: 15m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            summary: "vmalert instance {{ $labels.instance }} is missing rules evaluations"
            description: "vmalert instance {{ $labels.instance }} is missing rules evaluations for group \"{{ $labels.group }}\".
              The group evaluation time takes longer than the configured evaluation interval. This may result in missed
              alerting notifications or recording rules samples. Try increasing evaluation interval or concurrency of
              group \"{{ $labels.group }}\". See https://docs.victoriametrics.com/vmalert/#groups.
              If rule expressions are taking longer than expected, please see https://docs.victoriametrics.com/troubleshooting/#slow-queries."

        - alert: RemoteWriteErrors
          expr: increase(vmalert_remotewrite_errors_total{namespace="monitoring"}[5m]) > 0
          for: 15m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            summary: "vmalert instance {{ $labels.instance }} is failing to push metrics to remote write URL"
            description: "vmalert instance {{ $labels.instance }} is failing to push metrics generated via alerting
              or recording rules to the configured remote write URL. Check vmalert's logs for detailed error message."

        - alert: AlertmanagerErrors
          expr: increase(vmalert_alerts_send_errors_total{namespace="monitoring"}[5m]) > 0
          for: 15m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            summary: "vmalert instance {{ $labels.instance }} is failing to send notifications to Alertmanager"
            description: "vmalert instance {{ $labels.instance }} is failing to send alert notifications to \"{{ $labels.addr }}\".
              Check vmalert's logs for detailed error message."

    - name: vmagent
      interval: 30s
      concurrency: 2
      rules:

        - alert: PersistentQueueIsDroppingData
          expr: sum(increase(vm_persistentqueue_bytes_dropped_total{namespace="monitoring"}[5m])) by (job, instance) > 0
          for: 10m
          labels:
            severity: critical
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/dlE9IT7nk?viewPanel=49&var-instance={{ $labels.instance }}"
            summary: "Instance {{ $labels.instance }} is dropping data from persistent queue"
            description:
              "Vmagent dropped {{ $value | humanize1024 }} from persistent queue
              on instance {{ $labels.instance }} for the last 10m."

        - alert: RejectedRemoteWriteDataBlocksAreDropped
          expr: sum(increase(vmagent_remotewrite_packets_dropped_total{namespace="monitoring"}[5m])) without (url) > 0
          for: 15m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/G7Z9GzMGz?viewPanel=79&var-instance={{ $labels.instance }}"
            summary: "Job \"{{ $labels.job }}\" on instance {{ $labels.instance }} drops the rejected by
            remote-write server data blocks. Check the logs to find the reason for rejects."

        - alert: TooManyScrapeErrors
          expr: sum(increase(vm_promscrape_scrapes_failed_total{namespace="monitoring"}[5m])) by (job, instance) > 0
          for: 15m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/dlE9IT7nk?viewPanel=31&var-instance={{ $labels.instance }}"
            summary: 'Job "{{ $labels.job }}" on instance {{ $labels.instance }} fails to scrape targets for last 15m'

        - alert: TooManyWriteErrors
          expr: |
            (sum(increase(vm_ingestserver_request_errors_total{namespace="monitoring"}[5m])) by (job, instance)
            +
            sum(increase(vmagent_http_request_errors_total{namespace="monitoring"}[5m])) by (job, instance)) > 0
          for: 15m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/dlE9IT7nk?viewPanel=77&var-instance={{ $labels.instance }}"
            summary: 'Job "{{ $labels.job }}" on instance {{ $labels.instance }} responds with errors to write requests for last 15m.'

        - alert: TooManyRemoteWriteErrors
          expr: sum(rate(vmagent_remotewrite_retries_count_total{namespace="monitoring"}[5m])) by(job, instance, url) > 0
          for: 15m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/dlE9IT7nk?viewPanel=61&var-instance={{ $labels.instance }}"
            summary: 'Job "{{ $labels.job }}" on instance {{ $labels.instance }} fails to push to remote storage'
            description:
              "Vmagent fails to push data via remote write protocol to destination \"{{ $labels.url }}\"\n
              Ensure that destination is up and reachable."

        - alert: RemoteWriteConnectionIsSaturated
          expr: |
            (
            rate(vmagent_remotewrite_send_duration_seconds_total{namespace="monitoring"}[5m])
            /
            vmagent_remotewrite_queues{namespace="monitoring"}
            ) > 0.9
          for: 15m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/G7Z9GzMGz?viewPanel=84&var-instance={{ $labels.instance }}"
            summary: "Remote write connection from \"{{ $labels.job }}\" (instance {{ $labels.instance }}) to {{ $labels.url }} is saturated"
            description: "The remote write connection between vmagent \"{{ $labels.job }}\" (instance {{ $labels.instance }}) and destination \"{{ $labels.url }}\"
              is saturated by more than 90% and vmagent won't be able to keep up.\n
              This usually means that `-remoteWrite.queues` command-line flag must be increased in order to increase
              the number of connections per each remote storage."

        - alert: PersistentQueueForWritesIsSaturated
          expr: rate(vm_persistentqueue_write_duration_seconds_total{namespace="monitoring"}[5m]) > 0.9
          for: 15m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/G7Z9GzMGz?viewPanel=98&var-instance={{ $labels.instance }}"
            summary: "Persistent queue writes for instance {{ $labels.instance }} are saturated"
            description: "Persistent queue writes for vmagent \"{{ $labels.job }}\" (instance {{ $labels.instance }})
              are saturated by more than 90% and vmagent won't be able to keep up with flushing data on disk.
              In this case, consider to decrease load on the vmagent or improve the disk throughput."

        - alert: PersistentQueueForReadsIsSaturated
          expr: rate(vm_persistentqueue_read_duration_seconds_total{namespace="monitoring"}[5m]) > 0.9
          for: 15m
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            dashboard: "{{ $externalURL }}/d/G7Z9GzMGz?viewPanel=99&var-instance={{ $labels.instance }}"
            summary: "Persistent queue reads for instance {{ $labels.instance }} are saturated"
            description: "Persistent queue reads for vmagent \"{{ $labels.job }}\" (instance {{ $labels.instance }})
              are saturated by more than 90% and vmagent won't be able to keep up with reading data from the disk.
              In this case, consider to decrease load on the vmagent or improve the disk throughput."

        - alert: ConfigurationReloadFailure
          expr: |
            vm_promscrape_config_last_reload_successful{namespace="monitoring"} != 1
            or
            vmagent_relabel_config_last_reload_successful{namespace="monitoring"} != 1
          labels:
            severity: warning
            instance: "{{ $labels.instance }}"
            service: "{{ $labels.job }}"
          annotations:
            summary: "Configuration reload failed for vmagent instance {{ $labels.instance }}"
            description: "Configuration hot-reload failed for vmagent on instance {{ $labels.instance }}.
            Check vmagent's logs for detailed error message."
